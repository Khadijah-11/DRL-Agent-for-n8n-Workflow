# ğŸ§  DRL Agent for n8n Workflow Correction

This project implements a Deep Reinforcement Learning (DRL) agent designed to automatically detect and correct errors in n8n workflows.
It combines TensorFlow, Stable-Baselines3 (PPO), and Streamlit to provide both backend training and an interactive frontend interface.

## ğŸš€ Features

âœ… Reinforcement Learning (PPO) agent trained on workflow correction tasks

âœ… TensorBoard integration for monitoring training performance

âœ… Streamlit interface for interactive testing and visualization

âœ… n8n workflow parsing and fixing with automated correction suggestions

âœ… Modular code structure for easy customization and extension

## ğŸ“‚ Project Structure
DRL_Agent_N8N-main/
â”‚â”€â”€ app.py              # Streamlit UI  
â”‚â”€â”€ train.py            # DRL agent training loop  
â”‚â”€â”€ agent/              # DRL agent implementation  
â”‚â”€â”€ workflows/          # Example n8n workflows (training/test data)  
â”‚â”€â”€ logs/               # TensorBoard logs  
â”‚â”€â”€ requirements.txt    # Python dependencies  
â”‚â”€â”€ README.md           # Project documentation  

## âš¡ Quickstart
### 1ï¸âƒ£ Create and activate a virtual environment
python -m venv .env
source .env/bin/activate   # (Linux/Mac)
.env\Scripts\activate      # (Windows)

### 2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

### 3ï¸âƒ£ Run TensorBoard (monitor training)
tensorboard --logdir=./logs/ --port=6006


Then open: http://localhost:6006

### 4ï¸âƒ£ Launch the Streamlit app
streamlit run app.py



## ğŸ¥ Demo

Hereâ€™s a walkthrough of the Streamlit interface:

https://github.com/user-attachments/assets/69915613-dea4-4088-83b4-d40f5cf2a2c4

## ğŸ“Š Training with PPO

We use Proximal Policy Optimization (PPO) from Stable-Baselines3:

Reward function encourages valid workflow corrections

Logging and evaluation are tracked via TensorBoard

Training scripts are fully configurable in train.py


## ğŸ‘¥ Authors

Khadija Tagui &
Nisrin Lasfer
